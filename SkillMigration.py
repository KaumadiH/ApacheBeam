# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mxEH0-naxSazeQTf7_nCUHLWUxXmASWF
"""

!pip install --upgrade apache-beam

!pip install apache-beam[gcp]

import argparse
import apache_beam as beam
from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem
from apache_beam.options.pipeline_options import PipelineOptions, GoogleCloudOptions
from datetime import datetime
import json
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

class ExtractSkillsAndLocation(beam.DoFn):
    def process(self, element):
        country_code = element['Country-code']
        country_name = element['Country-name']
        wb_income = element['wb-income']
        wb_region = element['wb-region']
        skill_group_id = element['skill-group-id']
        skill_group_category = element['skill_group_category']
        skill_group_name = element['skill_group_name']
        net_per_10K_2015 = element['net_per_10K_2015']
        net_per_10K_2016 = element['net_per_10K_2016']
        net_per_10K_2017 = element['net_per_10K_2017']

        yield (country_code, country_name, wb_income, wb_region, skill_group_id,
               skill_group_category, skill_group_name, net_per_10K_2015,
               net_per_10K_2016, net_per_10K_2017)

def run():

    parser = argparse.ArgumentParser()
    args, beam_args = parser.parse_known_args()

    Timestamp = datetime.now()
    fileNameTimestamp_str = Timestamp.strftime("%Y%m%d%H%M%S")

    # Define pipeline options.
    options = PipelineOptions(beam_args,
                          runner='DataflowRunner',
                          project='astute-coda-422607-s7',
                          region='asia-southeast1-a', # Update region if needed
                          staging_location='gs://crosscutdataplatform/staging',
                          temp_location='gs://crosscutdataplatform/temp')

    # Create the Pipeline object.
    with beam.Pipeline(options=options) as pipeline:

        # Define the input file path.
        input_file = 'gs://crosscutdataplatform/input/skill_migration_public.csv'

        # Read data from the GCS file.
        data = pipeline | 'ReadData' >> beam.io.ReadFromText(input_file, skip_header_lines=1)

        # Parse CSV records and extract relevant fields.
        parsed_data = (
            data
            | 'ParseCSV' >> beam.Map(lambda row: row.split(',')) # Assuming CSV format
            | 'ExtractFields' >> beam.ParDo(ExtractSkillsAndLocation())
        )

       # Filter out records with missing or invalid data.
        filtered_data = (
            parsed_data
            | 'FilterData' >> beam.Filter(lambda record: record[0] is not None and record[6] is not None)
        )

        # Count occurrences of each skill group.
        skill_counts = (
            filtered_data
            | 'CountSkills' >> beam.Map(lambda record: (record[6], 1))
            | 'GroupBySkill' >> beam.CombinePerKey(sum)
        )

        # Calculate average income per skill group.
        avg_income_per_skill = (
            filtered_data
            | 'MapIncome' >> beam.Map(lambda record: (record[6], (float(record[7]) + float(record[8]) + float(record[9])) / 3))
            | 'GroupBySkillForAvgIncome' >> beam.CombinePerKey(beam.combiners.MeanCombineFn())
        )

        # Write results to output files in GCS.
        skill_counts | 'WriteSkillCounts' >> beam.io.WriteToText('gs://crosscutdataplatform/output/skill_counts')
        avg_income_per_skill | 'WriteAvgIncomePerSkill' >> beam.io.WriteToText('gs://crosscutdataplatform/output/avg_income_per_skill')

        # Collect results into a Pandas DataFrame for visualization.
        skill_counts_df = pd.DataFrame(skill_counts, columns=['Skill', 'Count'])
        avg_income_per_skill_df = pd.DataFrame(avg_income_per_skill, columns=['Skill', 'Average Income'])

        # Visualization: Bar chart for in-demand skills.
        plt.figure(figsize=(10, 6))
        sns.barplot(x='Count', y='Skill', data=skill_counts_df.nlargest(10, 'Count'))
        plt.title('Top 10 In-Demand Skills for Migration-Related Jobs')
        plt.xlabel('Count')
        plt.ylabel('Skill')
        plt.tight_layout()
        plt.savefig('in_demand_skills.png')
        plt.close()

        # Visualization: Choropleth map for geographic distribution of skills migration jobs (using Plotly).
        geo_data = {'Country': ['USA', 'Canada', 'Mexico'], 'Value': [100, 200, 150]}
        value_column = 'Value'
        fig = px.choropleth(geo_data, locations='Country', locationmode='country names', color=value_column,
                            color_continuous_scale='Viridis', range_color=(0, 300),
                            labels={value_column: 'Skills Migration Jobs'})
        fig.update_layout(title_text='Geographic Distribution of Skills Migration Jobs')
        fig.write_image('geographic_distribution.png')

if __name__ == '__main__':
    run()